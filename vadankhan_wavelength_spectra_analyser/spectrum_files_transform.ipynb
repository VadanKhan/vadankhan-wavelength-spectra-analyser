{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# import threading\n",
    "# import multiprocessing\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "RAW_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "EXPORTS_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('c:/Users/762093/OneDrive - Seagate Technology/Documents/LIV/vadankhan-wavelength-spectra-analyser/wavelength_spectra_files/QCHZZ_spectra.csv'), WindowsPath('c:/Users/762093/OneDrive - Seagate Technology/Documents/LIV/vadankhan-wavelength-spectra-analyser/wavelength_spectra_files/QCI1M_spectra.csv')]\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\"QCHZZ\", \"QCI1M\"]  # List of wafer codes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"SMSR_QC_FIRSTRUN\"\n",
    "\n",
    "SUBARU_DECODER = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "HALO_DECODER = \"HALO_DECODER_NE-rev1_1 logic_coords_annotated.csv\"\n",
    "\n",
    "\n",
    "def find_files_with_wafer_codes(wafer_codes, raw_file_path):\n",
    "    # Initialize an empty list to store the paths of the files\n",
    "    file_paths = []\n",
    "    # Iterate over each wafer code in the list\n",
    "    for code in wafer_codes:\n",
    "        # Iterate over each file in the raw file path directory\n",
    "        for root, dirs, files in os.walk(raw_file_path):\n",
    "            for file in files:\n",
    "                # Check if the wafer code is in the file name\n",
    "                if code in file:\n",
    "                    # Add the full path of the file to the list\n",
    "                    file_paths.append(Path(root) / file)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "# Call the function and print the results\n",
    "\n",
    "file_paths = find_files_with_wafer_codes(wafer_codes, RAW_FILE_PATH)\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting full processing run ===\n",
      "\n",
      "c:\\Users\\762093\\OneDrive - Seagate Technology\\Documents\\LIV\\vadankhan-wavelength-spectra-analyser\\exports\n",
      "\n",
      "--- Processing wafer: QCHZZ (Product: QC) ---\n",
      "Loading decoder from: c:\\Users\\762093\\OneDrive - Seagate Technology\\Documents\\LIV\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.88 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QCHZZ ===\n",
      "Starting file transformation for QCHZZ...\n",
      "File transformation for QCHZZ completed in 2733.49 seconds.\n",
      "=== Completed processing QCHZZ in 2733.49 seconds ===\n",
      "\n",
      "--- Processing wafer: QCI1M (Product: QC) ---\n",
      "Loading decoder from: c:\\Users\\762093\\OneDrive - Seagate Technology\\Documents\\LIV\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.60 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QCI1M ===\n",
      "Starting file transformation for QCI1M...\n",
      "File transformation for QCI1M completed in 2481.76 seconds.\n",
      "=== Completed processing QCI1M in 2481.76 seconds ===\n",
      "\n",
      "=== Total processing time: 5216.74 seconds ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decoder loading function\n",
    "def load_decoder(decoder_file_path):\n",
    "    print(f\"Loading decoder from: {decoder_file_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not decoder_file_path.exists():\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_decoder = pd.read_csv(decoder_file_path, usecols=[\"Logic_X\", \"Logic_Y\", \"TE_LABEL\", \"TYPE\"])\n",
    "    df_decoder = df_decoder.set_index([\"Logic_X\", \"Logic_Y\"])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Loaded in {end_time - start_time:.2f} seconds.\\n\")\n",
    "    return df_decoder\n",
    "\n",
    "\n",
    "# Function to transform raw file and merge with decoder data\n",
    "def transform_raw_file(filepath, wafer_id, decoder_df, wavelength_lb=824, wavelength_ub=832, chunksize=1000, max_chunks=400):\n",
    "    \"\"\"\n",
    "    Processes the raw file in chunks and returns a generator of transformed chunk dataframes,\n",
    "    along with the number of selected intensity columns.\n",
    "    \"\"\"\n",
    "    print(f\"Starting file transformation for {wafer_id}...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Read column names from the file header\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "    # Determine the number of selected intensity columns\n",
    "    data_points_threshold = len(selected_intensity_cols)\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break\n",
    "\n",
    "            # Reshape the data into long format (one row per measurement)\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "            # Convert the Wavelength column from string to actual float values\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "\n",
    "            # Merge with the decoder data using the spatial coordinates\n",
    "            long_df = long_df.merge(decoder_df, left_on=[\"X\", \"Y\"], right_index=True, how=\"left\")\n",
    "\n",
    "            # Drop raw coordinate columns and reorder the columns\n",
    "            long_df = long_df.drop(columns=[\"X\", \"Y\"])\n",
    "            long_df = long_df[[\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\"]]\n",
    "\n",
    "            yield long_df, data_points_threshold\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"File transformation for {wafer_id} completed in {t1 - t0:.2f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_top_two_peaks(df_group):\n",
    "    \"\"\"\n",
    "    Detects the top two peaks in a spectrum.\n",
    "    - SMSR_dB is calculated as -secondary_peak_dB (with main peak at 0 dB)\n",
    "    - SMSR_linear is calculated from the ratio of linear intensities\n",
    "    \"\"\"\n",
    "    df_sorted = df_group.sort_values(\"Wavelength\")\n",
    "    intensities = df_sorted[\"Intensity\"].values\n",
    "    dB_intensities = df_sorted[\"dB_Intensity\"].values\n",
    "    wavelengths = df_sorted[\"Wavelength\"].values\n",
    "\n",
    "    peak_indices, _ = find_peaks(dB_intensities)\n",
    "    if len(peak_indices) == 0:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"highest_peak_wavelength\": np.nan,\n",
    "                \"highest_peak_intensity_linear\": np.nan,\n",
    "                \"second_peak_wavelength\": np.nan,\n",
    "                \"second_peak_intensity_linear\": np.nan,\n",
    "                \"SMSR_dB\": np.nan,\n",
    "                \"SMSR_linear\": np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort peaks by dB intensity descending\n",
    "    sorted_order = np.argsort(dB_intensities[peak_indices])[::-1]\n",
    "    highest_idx = peak_indices[sorted_order[0]]\n",
    "    highest_peak_wavelength = wavelengths[highest_idx]\n",
    "    highest_peak_intensity_linear = intensities[highest_idx]\n",
    "\n",
    "    if len(sorted_order) > 1:\n",
    "        second_idx = peak_indices[sorted_order[1]]\n",
    "        second_peak_wavelength = wavelengths[second_idx]\n",
    "        second_peak_intensity_linear = intensities[second_idx]\n",
    "        second_peak_dB = dB_intensities[second_idx]\n",
    "\n",
    "        SMSR_dB = -second_peak_dB\n",
    "        SMSR_linear = highest_peak_intensity_linear / second_peak_intensity_linear\n",
    "    else:\n",
    "        second_peak_wavelength = np.nan\n",
    "        second_peak_intensity_linear = np.nan\n",
    "        SMSR_dB = np.nan\n",
    "        SMSR_linear = np.nan\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"highest_peak_wavelength\": highest_peak_wavelength,\n",
    "            \"highest_peak_intensity_linear\": highest_peak_intensity_linear,\n",
    "            \"second_peak_wavelength\": second_peak_wavelength,\n",
    "            \"second_peak_intensity_linear\": second_peak_intensity_linear,\n",
    "            \"SMSR_dB\": SMSR_dB,\n",
    "            \"SMSR_linear\": SMSR_linear,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def process_export_and_peaks(filepath, wafer_code, decoder_df):\n",
    "    \"\"\"\n",
    "    Processes the raw file in chunks, exports the transformed data with dB intensity,\n",
    "    and performs peak detection when sufficient data points have been accumulated for each TE_LABEL.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting processing for {wafer_code} ===\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # Output path for main spectra data\n",
    "    spectra_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    peak_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_peaks_summary.csv\"\n",
    "    first_chunk = True\n",
    "    first_peak = True\n",
    "\n",
    "    # Initialize buffers\n",
    "    accumulator = {}\n",
    "    data_point_count = {}\n",
    "\n",
    "    # Stream file in chunks\n",
    "    for chunk, data_points_threshold in transform_raw_file(filepath, wafer_code, decoder_df):\n",
    "        # Accumulate by TE_LABEL\n",
    "        for te_label, group in chunk.groupby(\"TE_LABEL\"):\n",
    "            if te_label not in accumulator:\n",
    "                accumulator[te_label] = [group]\n",
    "                data_point_count[te_label] = len(group)\n",
    "            else:\n",
    "                accumulator[te_label].append(group)\n",
    "                data_point_count[te_label] += len(group)\n",
    "\n",
    "            # Check if all data for this TE_LABEL has been accumulated\n",
    "            if data_point_count[te_label] >= data_points_threshold:\n",
    "                # Concatenate all accumulated data for this TE_LABEL\n",
    "                full_data = pd.concat(accumulator[te_label], ignore_index=True)\n",
    "\n",
    "                # Compute the maximum intensity for this TE_LABEL\n",
    "                max_intensity = full_data[\"Intensity\"].max()\n",
    "\n",
    "                # Calculate dB Intensity\n",
    "                full_data[\"dB_Intensity\"] = 10 * np.log10(full_data[\"Intensity\"] / max_intensity)\n",
    "\n",
    "                # Export the processed data with dB intensity\n",
    "                full_data.to_csv(spectra_output_path, mode=\"w\" if first_chunk else \"a\", header=first_chunk, index=False)\n",
    "                first_chunk = False\n",
    "\n",
    "                # Perform peak detection\n",
    "                peak_series = extract_top_two_peaks(full_data)\n",
    "                peak_series[\"TE_LABEL\"] = te_label\n",
    "\n",
    "                # Append peak data to the peak summary file\n",
    "                pd.DataFrame([peak_series]).to_csv(peak_output_path, mode=\"w\" if first_peak else \"a\", header=first_peak, index=False)\n",
    "                first_peak = False\n",
    "\n",
    "                # Clear buffer for this TE_LABEL\n",
    "                del accumulator[te_label]\n",
    "                del data_point_count[te_label]\n",
    "\n",
    "    total_t1 = time.time()\n",
    "    print(f\"=== Completed processing {wafer_code} in {total_t1 - total_t0:.2f} seconds ===\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "start_total_time = time.time()\n",
    "print(\"\\n=== Starting full processing run ===\\n\")\n",
    "\n",
    "print(EXPORTS_FILE_PATH)\n",
    "\n",
    "# file_paths should be defined elsewhere, assumed to match wafer_codes order\n",
    "for filepath, wafer_code in zip(file_paths, wafer_codes):\n",
    "    # Extract product code from the first two characters of the wafer code\n",
    "    product_code = wafer_code[:2]\n",
    "\n",
    "    print(f\"\\n--- Processing wafer: {wafer_code} (Product: {product_code}) ---\")\n",
    "\n",
    "    # Select decoder file based on product code\n",
    "    if product_code == \"QC\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / SUBARU_DECODER\n",
    "    elif product_code == \"QD\" or \"NV\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / HALO_DECODER\n",
    "    else:\n",
    "        print(f\"Unsupported product code: {product_code}\")\n",
    "        continue\n",
    "\n",
    "    # Load decoder and process\n",
    "    decoder_dict = load_decoder(decoder_path)\n",
    "    process_export_and_peaks(filepath, wafer_code, decoder_dict)\n",
    "\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_time = end_total_time - start_total_time\n",
    "\n",
    "\n",
    "print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Sort in Python (slower than JMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final output file by TE_LABEL\n",
    "def sort_large_csv_with_dask(input_path, output_path):\n",
    "    print(f\"Starting Dask sort for: {input_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read CSV with Dask\n",
    "    df = dd.read_csv(input_path, assume_missing=True)  # assume_missing=True is safe for mixed data\n",
    "\n",
    "    # Sort by TE_LABEL first, then Wavelength\n",
    "    df_sorted = df.sort_values(by=[\"TE_LABEL\", \"Wavelength\"])\n",
    "\n",
    "    # Save to CSV (can write to multiple files if very large)\n",
    "    df_sorted.to_csv(output_path, index=False, single_file=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Sorting and export completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# Call for each wafer (in case of multiple)\n",
    "for wafer_code in wafer_codes:\n",
    "    input_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    output_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_sorted.csv\"\n",
    "    sort_large_csv_with_dask(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Threading Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_RUN_NAME = \"larger\"\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, wavelength_lb=827, wavelength_ub=830, chunksize=1000, max_chunks=1000):\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns  # Read just headers\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "\n",
    "    # Extract wavelengths from column names\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "\n",
    "    # Filter columns to only include those within the desired range\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "\n",
    "    # Define columns to read\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break  # Stop after processing max_chunks\n",
    "\n",
    "            # Melt the dataframe: Convert wide format to long format\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "\n",
    "            # Convert \"Wavelength\" column from \"Intensity_xxx\" to just \"xxx\"\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "\n",
    "            yield long_df  # Yield processed chunk instead of storing in memory\n",
    "\n",
    "\n",
    "def process_and_export(filepath, wafer_code, export_path, run_name):\n",
    "    try:\n",
    "        print(f\"Processing file: {filepath} for wafer: {wafer_code}\")\n",
    "        output_path = export_path / f\"{run_name}_threaded_{wafer_code}_spectra_formatted.csv\"\n",
    "        first_chunk = True\n",
    "\n",
    "        for transformed_chunk in transform_raw_file(filepath, wafer_code):\n",
    "            # export_start_time = time.time()\n",
    "            transformed_chunk.to_csv(output_path, mode=\"w\" if first_chunk else \"a\", header=first_chunk, index=False)\n",
    "            first_chunk = False  # Only write header for the first chunk\n",
    "            # print(f\"Chunk exported for wafer {wafer_code}. Time taken: {time.time() - export_start_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "def parallel_processing(filepaths, wafer_codes, export_path, run_name):\n",
    "    print(f\"Starting parallel processing with {len(filepaths)} files.\")\n",
    "    # Use ThreadPoolExecutor instead of ProcessPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_and_export, f, w, export_path, run_name): (f, w) for f, w in zip(filepaths, wafer_codes)}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            filepath, wafer_code = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exception raised in the process\n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {filepath}, wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "# CALLING THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    start_total_time = time.time()\n",
    "    print(\"\\n=== Starting full processing run with multiprocessing ===\\n\")\n",
    "    parallel_processing(file_paths, wafer_codes, EXPORTS_FILE_PATH, ANALYSIS_RUN_NAME)\n",
    "    end_total_time = time.time()\n",
    "    total_time = end_total_time - start_total_time\n",
    "    print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: Spectrum Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sweep_data(df_raw_sweeps, wafer_code):\n",
    "    cod_roll_eval_types = [\"COD\", \"ROLLOVER\", \"NO LASER\"]\n",
    "    colors = [\"red\", \"blue\", \"orange\"]\n",
    "\n",
    "    # Scatter plot of PD against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"PD\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of PD vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"PD\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot DP/DI against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"DP/DI\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of DP/DI vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"DP/DI\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of Vf against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"Vf\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of Vf vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"Vf\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot dV/dI against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"dV/dI\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of dV/dI vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"dV/dI\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calling Code\n",
    "for df_raw_sweeps in annotated_sweeps_tables:\n",
    "    wafer_code = df_raw_sweeps[\"WAFER_ID\"].iloc[0]\n",
    "    plot_sweep_data(df_raw_sweeps, wafer_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
