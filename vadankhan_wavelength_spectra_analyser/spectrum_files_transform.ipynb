{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# import threading\n",
    "# import multiprocessing\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.csv as csv\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "RAW_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "EXPORTS_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('c:/Users/762093/Documents/vadankhan-wavelength-spectra-analyser/wavelength_spectra_files/QCI0Q_spectra.csv')]\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\"QCI0Q\"]  # List of wafer codes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"peak_detection_optimising\"\n",
    "\n",
    "SUBARU_DECODER = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "HALO_DECODER = \"HALO_DECODER_NE-rev1_1 logic_coords_annotated.csv\"\n",
    "\n",
    "\n",
    "def find_files_with_wafer_codes(wafer_codes, raw_file_path):\n",
    "    # Initialize an empty list to store the paths of the files\n",
    "    file_paths = []\n",
    "    # Iterate over each wafer code in the list\n",
    "    for code in wafer_codes:\n",
    "        # Iterate over each file in the raw file path directory\n",
    "        for root, dirs, files in os.walk(raw_file_path):\n",
    "            for file in files:\n",
    "                # Check if the wafer code is in the file name\n",
    "                if code in file:\n",
    "                    # Add the full path of the file to the list\n",
    "                    file_paths.append(Path(root) / file)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "# Call the function and print the results\n",
    "\n",
    "file_paths = find_files_with_wafer_codes(wafer_codes, RAW_FILE_PATH)\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting full processing run ===\n",
      "\n",
      "c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\exports\n",
      "\n",
      "--- Processing wafer: QCI0Q (Product: QC) ---\n",
      "Loading decoder from: c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.78 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QCI0Q ===\n",
      "Starting file transformation for QCI0Q...\n",
      "Header parsing and column filtering took 0.07 s\n",
      "Chunk 1 Summary:\n",
      "  Base transform: 0.10s\n",
      "  dB Calculation: 0.57s\n",
      "  Peak Calculation: 0.81s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.26s\n",
      "    find_peaks: 0.16s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.36s\n",
      "  Actual writing time: 3.48s\n",
      "  Chunk total:    5.39s\n",
      "\n",
      "File transformation for QCI0Q completed in 5.73 seconds.\n",
      "=== Completed processing QCI0Q in 5.74 seconds ===\n",
      "\n",
      "=== Total processing time: 6.53 seconds ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decoder loading function\n",
    "def load_decoder(decoder_file_path):\n",
    "    print(f\"Loading decoder from: {decoder_file_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not decoder_file_path.exists():\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_decoder = pd.read_csv(decoder_file_path, usecols=[\"Logic_X\", \"Logic_Y\", \"TE_LABEL\", \"TYPE\"])\n",
    "    df_decoder = df_decoder.set_index([\"Logic_X\", \"Logic_Y\"])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Loaded in {end_time - start_time:.2f} seconds.\\n\")\n",
    "    return df_decoder\n",
    "\n",
    "\n",
    "def get_columns_in_wavelength_range(filepath, wavelength_lb, wavelength_ub):\n",
    "    # Read just the header row\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "\n",
    "    # Find columns that are within the desired wavelength range\n",
    "    selected_intensity_cols = [col for col in header if col.startswith(\"Intensity_\") and wavelength_lb <= float(col.split(\"_\")[1]) <= wavelength_ub]\n",
    "\n",
    "    # Add essential metadata columns\n",
    "    return [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "\n",
    "def append_table_to_csv(table, output_path, include_header):\n",
    "    sink = io.BytesIO()\n",
    "    csv.write_csv(table, sink, write_options=csv.WriteOptions(include_header=include_header))\n",
    "    sink.seek(0)\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(sink.read().decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, decoder_df, wavelength_lb=824, wavelength_ub=832, chunksize=1000, max_chunks=1):\n",
    "    print(f\"Starting file transformation for {wafer_id}...\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    t1 = time.time()\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "    data_points_threshold = len(selected_intensity_cols)\n",
    "    print(f\"Header parsing and column filtering took {time.time() - t1:.2f} s\")\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break\n",
    "\n",
    "            chunk_start = time.time()\n",
    "\n",
    "            # Base transformation\n",
    "            t_base = time.time()\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "            long_df = long_df.merge(decoder_df, left_on=[\"X\", \"Y\"], right_index=True, how=\"left\")\n",
    "            long_df = long_df.drop(columns=[\"X\", \"Y\"])\n",
    "            long_df = long_df[[\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\"]]\n",
    "            t_base_elapsed = time.time() - t_base\n",
    "\n",
    "            yield long_df, data_points_threshold, t_base_elapsed\n",
    "\n",
    "            # print(f\"Chunk {i+1} | Base transform: {t_base_elapsed:.2f}s | Total time: {time.time() - chunk_start:.2f}s\")\n",
    "\n",
    "    print(f\"File transformation for {wafer_id} completed in {time.time() - total_t0:.2f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_top_two_peaks(df_group):\n",
    "    \"\"\"\n",
    "    Detects the top two peaks in a spectrum.\n",
    "    Returns:\n",
    "        peak_series (pd.Series): Summary of top peaks and SMSR.\n",
    "        timing_info (dict): Time taken for each step.\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "    timing = {}\n",
    "\n",
    "    t0 = time.time()\n",
    "    df_sorted = df_group.sort_values(\"Wavelength\")\n",
    "    timing[\"sort\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    intensities = df_sorted[\"Intensity\"].values\n",
    "    dB_intensities = df_sorted[\"dB_Intensity\"].values\n",
    "    wavelengths = df_sorted[\"Wavelength\"].values\n",
    "    peak_indices, _ = find_peaks(dB_intensities)\n",
    "    timing[\"find_peaks\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    if len(peak_indices) == 0:\n",
    "        peak_series = pd.Series(\n",
    "            {\n",
    "                \"highest_peak_wavelength\": np.nan,\n",
    "                \"highest_peak_intensity_linear\": np.nan,\n",
    "                \"second_peak_wavelength\": np.nan,\n",
    "                \"second_peak_intensity_linear\": np.nan,\n",
    "                \"SMSR_dB\": np.nan,\n",
    "                \"SMSR_linear\": np.nan,\n",
    "            }\n",
    "        )\n",
    "        timing[\"ordering\"] = 0.0\n",
    "        timing[\"extraction\"] = 0.0\n",
    "        return peak_series, timing\n",
    "\n",
    "    sorted_order = np.argsort(dB_intensities[peak_indices])[::-1]\n",
    "    timing[\"ordering\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    highest_idx = peak_indices[sorted_order[0]]\n",
    "    highest_peak_wavelength = wavelengths[highest_idx]\n",
    "    highest_peak_intensity_linear = intensities[highest_idx]\n",
    "\n",
    "    if len(sorted_order) > 1:\n",
    "        second_idx = peak_indices[sorted_order[1]]\n",
    "        second_peak_wavelength = wavelengths[second_idx]\n",
    "        second_peak_intensity_linear = intensities[second_idx]\n",
    "        second_peak_dB = dB_intensities[second_idx]\n",
    "\n",
    "        SMSR_dB = -second_peak_dB\n",
    "        SMSR_linear = highest_peak_intensity_linear / second_peak_intensity_linear\n",
    "    else:\n",
    "        second_peak_wavelength = np.nan\n",
    "        second_peak_intensity_linear = np.nan\n",
    "        SMSR_dB = np.nan\n",
    "        SMSR_linear = np.nan\n",
    "\n",
    "    peak_series = pd.Series(\n",
    "        {\n",
    "            \"highest_peak_wavelength\": highest_peak_wavelength,\n",
    "            \"highest_peak_intensity_linear\": highest_peak_intensity_linear,\n",
    "            \"second_peak_wavelength\": second_peak_wavelength,\n",
    "            \"second_peak_intensity_linear\": second_peak_intensity_linear,\n",
    "            \"SMSR_dB\": SMSR_dB,\n",
    "            \"SMSR_linear\": SMSR_linear,\n",
    "        }\n",
    "    )\n",
    "    timing[\"extraction\"] = time.time() - t0\n",
    "\n",
    "    return peak_series, timing\n",
    "\n",
    "\n",
    "def process_export_and_peaks(filepath, wafer_code, decoder_df):\n",
    "    print(f\"\\n=== Starting processing for {wafer_code} ===\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # File paths for the exports\n",
    "    spectra_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    peak_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_peaks_summary.csv\"\n",
    "\n",
    "    first_chunk = True\n",
    "    first_peak = True\n",
    "\n",
    "    accumulator = {}\n",
    "    data_point_count = {}\n",
    "\n",
    "    chunk_counter = 0\n",
    "\n",
    "    # Column headers for the spectra file\n",
    "    spectra_columns = [\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\", \"dB_Intensity\"]\n",
    "    peak_colums = [\"Highest Peak (Wavelength)\", \"Highest Peak (Linear Intensity)\", \"Second Peak (Wavelength)\", \"Second Peak (Linear Intensity)\", \"SMSR_dB\", \"SMSR_linear\", \"TE_LABEL\"]\n",
    "\n",
    "    # Write headers first for both files\n",
    "    with open(spectra_output_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(spectra_columns)\n",
    "    with open(peak_output_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(peak_colums)\n",
    "\n",
    "    for chunk, data_points_threshold, base_time in transform_raw_file(filepath, wafer_code, decoder_df):\n",
    "        chunk_counter += 1\n",
    "        chunk_start = time.time()\n",
    "        t_peaks_breakdown = {}\n",
    "\n",
    "        t_peak_total = 0\n",
    "        t_actual_write_total = 0\n",
    "        t_dB_total = 0\n",
    "\n",
    "        for te_label, group in chunk.groupby(\"TE_LABEL\"):\n",
    "            if te_label not in accumulator:\n",
    "                accumulator[te_label] = [group]\n",
    "                data_point_count[te_label] = len(group)\n",
    "            else:\n",
    "                accumulator[te_label].append(group)\n",
    "                data_point_count[te_label] += len(group)\n",
    "\n",
    "            if data_point_count[te_label] >= data_points_threshold:\n",
    "                full_data = pd.concat(accumulator[te_label], ignore_index=True)\n",
    "\n",
    "                # Peak processing\n",
    "                t_dB_start = time.time()\n",
    "                max_intensity = full_data[\"Intensity\"].max()\n",
    "                # Avoid division by zero and log10(0) → use np.where to set zero to np.nan\n",
    "                safe_intensity = np.where(full_data[\"Intensity\"] > 0, full_data[\"Intensity\"], np.nan)\n",
    "                # Compute dB only for valid values\n",
    "                full_data[\"dB_Intensity\"] = 10 * np.log10(safe_intensity / max_intensity)\n",
    "                t_dB_end = time.time()\n",
    "                t_dB_total += t_dB_end - t_dB_start\n",
    "\n",
    "                # Extract top two peaks\n",
    "                t_peak_start = time.time()\n",
    "                peak_series, peak_times = extract_top_two_peaks(full_data)\n",
    "                t_peak_end = time.time()\n",
    "                t_peak_total += t_peak_end - t_peak_start\n",
    "\n",
    "                # Add timings to the respective totals\n",
    "                for k, v in peak_times.items():\n",
    "                    t_peaks_breakdown[k] = t_peaks_breakdown.get(k, 0.0) + v\n",
    "                peak_df = pd.DataFrame([peak_series])  # Convert peak series to DataFrame\n",
    "\n",
    "                # Actual writing of spectra data\n",
    "                t_actual_write_start = time.time()\n",
    "                full_data.to_csv(spectra_output_path, mode=\"a\", header=False, index=False)\n",
    "                peak_df.to_csv(peak_output_path, mode=\"a\", header=False, index=False)\n",
    "                t_actual_write_end = time.time()\n",
    "                t_actual_write_total += t_actual_write_end - t_actual_write_start\n",
    "\n",
    "                # Clean up to free memory\n",
    "                del accumulator[te_label]\n",
    "                del data_point_count[te_label]\n",
    "\n",
    "        chunk_total = time.time() - chunk_start\n",
    "        print(f\"Chunk {chunk_counter} Summary:\")\n",
    "        print(f\"  Base transform: {base_time:.2f}s\")\n",
    "        print(f\"  dB Calculation: {t_dB_total:.2f}s\")\n",
    "        print(f\"  Peak Calculation: {t_peak_total:.2f}s\")\n",
    "        print(f\"  Peak detection breakdown:\")\n",
    "        for step, t in t_peaks_breakdown.items():\n",
    "            print(f\"    {step:>10}: {t:.2f}s\")\n",
    "        print(f\"  Actual writing time: {t_actual_write_total:.2f}s\")\n",
    "        print(f\"  Chunk total:    {chunk_total:.2f}s\\n\")\n",
    "\n",
    "    print(f\"=== Completed processing {wafer_code} in {time.time() - total_t0:.2f} seconds ===\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "start_total_time = time.time()\n",
    "print(\"\\n=== Starting full processing run ===\\n\")\n",
    "\n",
    "print(EXPORTS_FILE_PATH)\n",
    "\n",
    "# file_paths should be defined elsewhere, assumed to match wafer_codes order\n",
    "for filepath, wafer_code in zip(file_paths, wafer_codes):\n",
    "    # Extract product code from the first two characters of the wafer code\n",
    "    product_code = wafer_code[:2]\n",
    "\n",
    "    print(f\"\\n--- Processing wafer: {wafer_code} (Product: {product_code}) ---\")\n",
    "\n",
    "    # Select decoder file based on product code\n",
    "    if product_code == \"QC\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / SUBARU_DECODER\n",
    "    elif product_code == \"QD\" or \"NV\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / HALO_DECODER\n",
    "    else:\n",
    "        print(f\"Unsupported product code: {product_code}\")\n",
    "        continue\n",
    "\n",
    "    # Load decoder and process\n",
    "    decoder_dict = load_decoder(decoder_path)\n",
    "    process_export_and_peaks(filepath, wafer_code, decoder_dict)\n",
    "\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_time = end_total_time - start_total_time\n",
    "\n",
    "\n",
    "print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Sort in Python (slower than JMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final output file by TE_LABEL\n",
    "def sort_large_csv_with_dask(input_path, output_path):\n",
    "    print(f\"Starting Dask sort for: {input_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read CSV with Dask\n",
    "    df = dd.read_csv(input_path, assume_missing=True)  # assume_missing=True is safe for mixed data\n",
    "\n",
    "    # Sort by TE_LABEL first, then Wavelength\n",
    "    df_sorted = df.sort_values(by=[\"TE_LABEL\", \"Wavelength\"])\n",
    "\n",
    "    # Save to CSV (can write to multiple files if very large)\n",
    "    df_sorted.to_csv(output_path, index=False, single_file=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Sorting and export completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# Call for each wafer (in case of multiple)\n",
    "for wafer_code in wafer_codes:\n",
    "    input_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    output_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_sorted.csv\"\n",
    "    sort_large_csv_with_dask(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Threading Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_RUN_NAME = \"larger\"\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, wavelength_lb=827, wavelength_ub=830, chunksize=1000, max_chunks=1000):\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns  # Read just headers\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "\n",
    "    # Extract wavelengths from column names\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "\n",
    "    # Filter columns to only include those within the desired range\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "\n",
    "    # Define columns to read\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break  # Stop after processing max_chunks\n",
    "\n",
    "            # Melt the dataframe: Convert wide format to long format\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "\n",
    "            # Convert \"Wavelength\" column from \"Intensity_xxx\" to just \"xxx\"\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "\n",
    "            yield long_df  # Yield processed chunk instead of storing in memory\n",
    "\n",
    "\n",
    "def process_and_export(filepath, wafer_code, export_path, run_name):\n",
    "    try:\n",
    "        print(f\"Processing file: {filepath} for wafer: {wafer_code}\")\n",
    "        output_path = export_path / f\"{run_name}_threaded_{wafer_code}_spectra_formatted.csv\"\n",
    "        first_chunk = True\n",
    "\n",
    "        for transformed_chunk in transform_raw_file(filepath, wafer_code):\n",
    "            # export_start_time = time.time()\n",
    "            transformed_chunk.to_csv(output_path, mode=\"w\" if first_chunk else \"a\", header=first_chunk, index=False)\n",
    "            first_chunk = False  # Only write header for the first chunk\n",
    "            # print(f\"Chunk exported for wafer {wafer_code}. Time taken: {time.time() - export_start_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "def parallel_processing(filepaths, wafer_codes, export_path, run_name):\n",
    "    print(f\"Starting parallel processing with {len(filepaths)} files.\")\n",
    "    # Use ThreadPoolExecutor instead of ProcessPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_and_export, f, w, export_path, run_name): (f, w) for f, w in zip(filepaths, wafer_codes)}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            filepath, wafer_code = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exception raised in the process\n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {filepath}, wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "# CALLING THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    start_total_time = time.time()\n",
    "    print(\"\\n=== Starting full processing run with multiprocessing ===\\n\")\n",
    "    parallel_processing(file_paths, wafer_codes, EXPORTS_FILE_PATH, ANALYSIS_RUN_NAME)\n",
    "    end_total_time = time.time()\n",
    "    total_time = end_total_time - start_total_time\n",
    "    print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
