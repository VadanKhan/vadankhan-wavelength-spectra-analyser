{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# import threading\n",
    "# import multiprocessing\n",
    "# import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.csv as csv\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "RAW_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "EXPORTS_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "\n",
    "SUBARU_DECODER = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "HALO_DECODER = \"HALO_DECODER_NE-rev1_1 logic_coords_annotated.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('d:/SPECTRUM ANALYSIS/vadankhan-wavelength-spectra-analyser/wavelength_spectra_files/QLJDI.csv')]\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\n",
    "    \"QLJDI\",\n",
    "]  # List of wafer codes\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"DBR_LASER\"\n",
    "\n",
    "\n",
    "# ---------------- Implementation Code (not for user to edit) ---------------- #\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def find_files_with_wafer_codes(wafer_codes, raw_file_path):\n",
    "    # Initialize an empty list to store the paths of the files\n",
    "    file_paths = []\n",
    "    # Iterate over each wafer code in the list\n",
    "    for code in wafer_codes:\n",
    "        # Iterate over each file in the raw file path directory\n",
    "        for root, dirs, files in os.walk(raw_file_path):\n",
    "            for file in files:\n",
    "                # Check if the wafer code is in the file name\n",
    "                if code in file:\n",
    "                    # Add the full path of the file to the list\n",
    "                    file_paths.append(Path(root) / file)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "# Call the function and print the results\n",
    "\n",
    "file_paths = find_files_with_wafer_codes(wafer_codes, RAW_FILE_PATH)\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum Analyser Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_decoder(decoder_file_path):\n",
    "    print(f\"Loading decoder from: {decoder_file_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not decoder_file_path.exists():\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_decoder = pd.read_csv(decoder_file_path, usecols=[\"Logic_X\", \"Logic_Y\", \"TE_LABEL\", \"TYPE\"])\n",
    "    df_decoder = df_decoder.set_index([\"Logic_X\", \"Logic_Y\"])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Loaded in {end_time - start_time:.2f} seconds.\\n\")\n",
    "    return df_decoder\n",
    "\n",
    "\n",
    "def transform_raw_file(\n",
    "    filepath,\n",
    "    wafer_id,\n",
    "    decoder_df,\n",
    "    wavelength_lb=700,\n",
    "    wavelength_ub=900,\n",
    "    chunksize=1000,\n",
    "    max_chunks=400,\n",
    "):\n",
    "    print(f\"Starting file transformation for {wafer_id}...\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    t1 = time.time()\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "    data_points_threshold = len(selected_intensity_cols)\n",
    "    print(f\"Header parsing and column filtering took {time.time() - t1:.2f} s\")\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break\n",
    "\n",
    "            chunk_start = time.time()\n",
    "\n",
    "            # Base transformation\n",
    "            t_base = time.time()\n",
    "            long_df = chunk.melt(\n",
    "                id_vars=[\"X\", \"Y\"],\n",
    "                value_vars=selected_intensity_cols,\n",
    "                var_name=\"Wavelength\",\n",
    "                value_name=\"Intensity\",\n",
    "            )\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "            long_df = long_df.merge(decoder_df, left_on=[\"X\", \"Y\"], right_index=True, how=\"left\")\n",
    "            long_df = long_df.drop(columns=[\"X\", \"Y\"])\n",
    "            long_df = long_df[[\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\"]]\n",
    "            t_base_elapsed = time.time() - t_base\n",
    "\n",
    "            yield long_df, data_points_threshold, t_base_elapsed\n",
    "\n",
    "            # print(f\"Chunk {i+1} | Base transform: {t_base_elapsed:.2f}s | Total time: {time.time() - chunk_start:.2f}s\")\n",
    "\n",
    "    print(f\"File transformation for {wafer_id} completed in {time.time() - total_t0:.2f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_top_two_peaks(df_group):\n",
    "    \"\"\"\n",
    "    Detects the top two peaks in a spectrum.\n",
    "    Returns:\n",
    "        peak_series (pd.Series): Summary of top peaks and SMSR.\n",
    "        timing_info (dict): Time taken for each step.\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "    timing = {}\n",
    "\n",
    "    t0 = time.time()\n",
    "    df_sorted = df_group.sort_values(\"Wavelength\")\n",
    "    timing[\"sort\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    intensities = df_sorted[\"Intensity\"].values\n",
    "    dB_intensities = df_sorted[\"dB_Intensity\"].values\n",
    "    wavelengths = df_sorted[\"Wavelength\"].values\n",
    "    peak_indices, _ = find_peaks(dB_intensities)\n",
    "    timing[\"find_peaks\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    if len(peak_indices) == 0:\n",
    "        peak_series = pd.Series(\n",
    "            {\n",
    "                \"highest_peak_wavelength\": np.nan,\n",
    "                \"highest_peak_intensity_linear\": np.nan,\n",
    "                \"second_peak_wavelength\": np.nan,\n",
    "                \"second_peak_intensity_linear\": np.nan,\n",
    "                \"SMSR_dB\": np.nan,\n",
    "                \"SMSR_linear\": np.nan,\n",
    "            }\n",
    "        )\n",
    "        timing[\"ordering\"] = 0.0\n",
    "        timing[\"extraction\"] = 0.0\n",
    "        return peak_series, timing\n",
    "\n",
    "    sorted_order = np.argsort(dB_intensities[peak_indices])[::-1]\n",
    "    timing[\"ordering\"] = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    highest_idx = peak_indices[sorted_order[0]]\n",
    "    highest_peak_wavelength = wavelengths[highest_idx]\n",
    "    highest_peak_intensity_linear = intensities[highest_idx]\n",
    "\n",
    "    if len(sorted_order) > 1:\n",
    "        second_idx = peak_indices[sorted_order[1]]\n",
    "        second_peak_wavelength = wavelengths[second_idx]\n",
    "        second_peak_intensity_linear = intensities[second_idx]\n",
    "        second_peak_dB = dB_intensities[second_idx]\n",
    "\n",
    "        SMSR_dB = -second_peak_dB\n",
    "        SMSR_linear = highest_peak_intensity_linear / second_peak_intensity_linear\n",
    "    else:\n",
    "        second_peak_wavelength = np.nan\n",
    "        second_peak_intensity_linear = np.nan\n",
    "        SMSR_dB = np.nan\n",
    "        SMSR_linear = np.nan\n",
    "\n",
    "    peak_series = pd.Series(\n",
    "        {\n",
    "            \"highest_peak_wavelength\": highest_peak_wavelength,\n",
    "            \"highest_peak_intensity_linear\": highest_peak_intensity_linear,\n",
    "            \"second_peak_wavelength\": second_peak_wavelength,\n",
    "            \"second_peak_intensity_linear\": second_peak_intensity_linear,\n",
    "            \"SMSR_dB\": SMSR_dB,\n",
    "            \"SMSR_linear\": SMSR_linear,\n",
    "        }\n",
    "    )\n",
    "    timing[\"extraction\"] = time.time() - t0\n",
    "\n",
    "    return peak_series, timing\n",
    "\n",
    "\n",
    "def process_export_and_peaks(filepath, wafer_code, decoder_df):\n",
    "    print(f\"\\n=== Starting processing for {wafer_code} ===\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    spectra_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_formatted_spectra.csv\"\n",
    "    peak_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_headlevel_SMSR.csv\"\n",
    "\n",
    "    accumulator = {}\n",
    "    data_point_count = {}\n",
    "    chunk_counter = 0\n",
    "\n",
    "    spectra_columns = [\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\", \"dB_Intensity\"]\n",
    "    peak_columns = [\n",
    "        \"Highest Peak (Wavelength)\",\n",
    "        \"Highest Peak (Linear Intensity)\",\n",
    "        \"Second Peak (Wavelength)\",\n",
    "        \"Second Peak (Linear Intensity)\",\n",
    "        \"SMSR_dB\",\n",
    "        \"SMSR_linear\",\n",
    "        \"TE_LABEL\",\n",
    "    ]\n",
    "\n",
    "    with open(spectra_output_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(spectra_columns)\n",
    "    with open(peak_output_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(peak_columns)\n",
    "\n",
    "    completed_labels = 0\n",
    "    spectra_buffer = []\n",
    "    peak_buffer = []\n",
    "\n",
    "    for chunk, data_points_threshold, base_time in transform_raw_file(filepath, wafer_code, decoder_df):\n",
    "        chunk_counter += 1\n",
    "        chunk_start = time.time()\n",
    "\n",
    "        t_peaks_breakdown = {}\n",
    "        t_peak_total = 0\n",
    "        t_actual_write_total = 0\n",
    "        t_dB_total = 0\n",
    "\n",
    "        for te_label, group in chunk.groupby(\"TE_LABEL\"):\n",
    "            if te_label not in accumulator:\n",
    "                accumulator[te_label] = [group]\n",
    "                data_point_count[te_label] = len(group)\n",
    "            else:\n",
    "                accumulator[te_label].append(group)\n",
    "                data_point_count[te_label] += len(group)\n",
    "\n",
    "            if data_point_count[te_label] >= data_points_threshold:\n",
    "                full_data = pd.concat(accumulator[te_label], ignore_index=True)\n",
    "\n",
    "                # dB calculation\n",
    "                t_dB_start = time.time()\n",
    "                max_intensity = full_data[\"Intensity\"].max()\n",
    "                safe_intensity = np.where(full_data[\"Intensity\"] > 0, full_data[\"Intensity\"], np.nan)\n",
    "                full_data[\"dB_Intensity\"] = 10 * np.log10(safe_intensity / max_intensity)\n",
    "                t_dB_end = time.time()\n",
    "                t_dB_total += t_dB_end - t_dB_start\n",
    "\n",
    "                # Peak extraction\n",
    "                t_peak_start = time.time()\n",
    "                peak_series, peak_times = extract_top_two_peaks(full_data)\n",
    "                t_peak_end = time.time()\n",
    "                t_peak_total += t_peak_end - t_peak_start\n",
    "\n",
    "                for k, v in peak_times.items():\n",
    "                    t_peaks_breakdown[k] = t_peaks_breakdown.get(k, 0.0) + v\n",
    "\n",
    "                peak_series[\"TE_LABEL\"] = te_label\n",
    "                peak_buffer.append(peak_series)\n",
    "                spectra_buffer.append(full_data)\n",
    "\n",
    "                completed_labels += 1\n",
    "                del accumulator[te_label]\n",
    "                del data_point_count[te_label]\n",
    "\n",
    "                # Flush if 1000 TE_LABELs completed\n",
    "                if completed_labels >= 1000:\n",
    "                    t_actual_write_start = time.time()\n",
    "                    pd.concat(spectra_buffer).to_csv(spectra_output_path, mode=\"a\", header=False, index=False)\n",
    "                    pd.DataFrame(peak_buffer).to_csv(peak_output_path, mode=\"a\", header=False, index=False)\n",
    "                    t_actual_write_end = time.time()\n",
    "                    t_actual_write_total += t_actual_write_end - t_actual_write_start\n",
    "\n",
    "                    spectra_buffer.clear()\n",
    "                    peak_buffer.clear()\n",
    "                    completed_labels = 0\n",
    "\n",
    "        chunk_total = time.time() - chunk_start\n",
    "        print(f\"Chunk {chunk_counter} Summary:\")\n",
    "        print(f\"  Base transform: {base_time:.2f}s\")\n",
    "        print(f\"  dB Calculation: {t_dB_total:.2f}s\")\n",
    "        print(f\"  Peak Calculation Total: {t_peak_total:.2f}s\")\n",
    "        print(f\"  Peak detection breakdown:\")\n",
    "        for step, t in t_peaks_breakdown.items():\n",
    "            print(f\"    {step:>10}: {t:.2f}s\")\n",
    "        print(f\"  Actual writing time: {t_actual_write_total:.2f}s\")\n",
    "        print(f\"  Chunk total:    {chunk_total:.2f}s\\n\")\n",
    "\n",
    "    # Final flush\n",
    "    if spectra_buffer:\n",
    "        pd.concat(spectra_buffer).to_csv(spectra_output_path, mode=\"a\", header=False, index=False)\n",
    "    if peak_buffer:\n",
    "        pd.DataFrame(peak_buffer).to_csv(peak_output_path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    print(f\"=== Completed processing {wafer_code} in {time.time() - total_t0:.2f} seconds ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local File Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting full processing run ===\n",
      "\n",
      "\n",
      "--- Processing wafer: QLJDI (Product: QL) ---\n",
      "Loading decoder from: d:\\SPECTRUM ANALYSIS\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.55 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QLJDI ===\n",
      "Starting file transformation for QLJDI...\n",
      "Header parsing and column filtering took 0.14 s\n"
     ]
    }
   ],
   "source": [
    "# Local File Execution\n",
    "# Main execution\n",
    "start_total_time = time.time()\n",
    "print(\"\\n=== Starting full processing run ===\\n\")\n",
    "\n",
    "# file_paths should be defined elsewhere, assumed to match wafer_codes order\n",
    "for filepath, wafer_code in zip(file_paths, wafer_codes):\n",
    "    # Extract product code from the first two characters of the wafer code\n",
    "    product_code = wafer_code[:2]\n",
    "\n",
    "    print(f\"\\n--- Processing wafer: {wafer_code} (Product: {product_code}) ---\")\n",
    "\n",
    "    # Select decoder file based on product code\n",
    "    if product_code in (\"QD\", \"NV\"):\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / HALO_DECODER\n",
    "    else:\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / SUBARU_DECODER\n",
    "\n",
    "    # Load decoder and process\n",
    "    decoder_dict = load_decoder(decoder_path)\n",
    "    process_export_and_peaks(filepath, wafer_code, decoder_dict)\n",
    "\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_time = end_total_time - start_total_time\n",
    "\n",
    "\n",
    "print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watchdog Montior Folder Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching folder: c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\monitored_folder\n",
      "\n",
      "Detected new file: QCI0Q_spectra.csv\n",
      "Waiting for file to be ready... (attempt 1)\n",
      "Waiting for file to be ready... (attempt 2)\n",
      "Waiting for file to be ready... (attempt 3)\n",
      "Waiting for file to be ready... (attempt 4)\n",
      "Waiting for file to be ready... (attempt 5)\n",
      "Waiting for file to be ready... (attempt 6)\n",
      "Waiting for file to be ready... (attempt 7)\n",
      "Extracted wafer code: QCI0Q\n",
      "Loading decoder from: c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.85 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QCI0Q ===\n",
      "Starting file transformation for QCI0Q...\n",
      "Header parsing and column filtering took 0.09 s\n",
      "Chunk 1 Summary:\n",
      "  Base transform: 0.12s\n",
      "  dB Calculation: 0.34s\n",
      "  Peak Calculation Total: 0.57s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.33s\n",
      "    find_peaks: 0.09s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.45s\n",
      "  Chunk total:    1.89s\n",
      "\n",
      "Chunk 2 Summary:\n",
      "  Base transform: 0.11s\n",
      "  dB Calculation: 0.38s\n",
      "  Peak Calculation Total: 0.59s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.35s\n",
      "    find_peaks: 0.10s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.53s\n",
      "  Chunk total:    2.03s\n",
      "\n",
      "Chunk 3 Summary:\n",
      "  Base transform: 0.12s\n",
      "  dB Calculation: 0.38s\n",
      "  Peak Calculation Total: 0.62s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.37s\n",
      "    find_peaks: 0.09s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.52s\n",
      "  Chunk total:    2.06s\n",
      "\n",
      "Chunk 4 Summary:\n",
      "  Base transform: 0.11s\n",
      "  dB Calculation: 0.37s\n",
      "  Peak Calculation Total: 0.44s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.24s\n",
      "    find_peaks: 0.09s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.10s\n",
      "  Actual writing time: 0.57s\n",
      "  Chunk total:    1.90s\n",
      "\n",
      "Chunk 5 Summary:\n",
      "  Base transform: 0.10s\n",
      "  dB Calculation: 0.37s\n",
      "  Peak Calculation Total: 0.52s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.28s\n",
      "    find_peaks: 0.11s\n",
      "      ordering: 0.00s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.44s\n",
      "  Chunk total:    1.92s\n",
      "\n",
      "Chunk 6 Summary:\n",
      "  Base transform: 0.12s\n",
      "  dB Calculation: 0.39s\n",
      "  Peak Calculation Total: 0.59s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.34s\n",
      "    find_peaks: 0.11s\n",
      "      ordering: 0.00s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.48s\n",
      "  Chunk total:    2.01s\n",
      "\n",
      "Chunk 7 Summary:\n",
      "  Base transform: 0.11s\n",
      "  dB Calculation: 0.35s\n",
      "  Peak Calculation Total: 0.53s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.31s\n",
      "    find_peaks: 0.10s\n",
      "      ordering: 0.00s\n",
      "    extraction: 0.11s\n",
      "  Actual writing time: 0.50s\n",
      "  Chunk total:    1.88s\n",
      "\n",
      "Chunk 8 Summary:\n",
      "  Base transform: 0.11s\n",
      "  dB Calculation: 0.35s\n",
      "  Peak Calculation Total: 0.54s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.31s\n",
      "    find_peaks: 0.09s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.13s\n",
      "  Actual writing time: 0.43s\n",
      "  Chunk total:    1.87s\n",
      "\n",
      "Chunk 9 Summary:\n",
      "  Base transform: 0.10s\n",
      "  dB Calculation: 0.32s\n",
      "  Peak Calculation Total: 0.46s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.23s\n",
      "    find_peaks: 0.10s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.12s\n",
      "  Actual writing time: 0.55s\n",
      "  Chunk total:    1.81s\n",
      "\n",
      "Chunk 10 Summary:\n",
      "  Base transform: 0.11s\n",
      "  dB Calculation: 0.34s\n",
      "  Peak Calculation Total: 0.46s\n",
      "  Peak detection breakdown:\n",
      "          sort: 0.22s\n",
      "    find_peaks: 0.09s\n",
      "      ordering: 0.01s\n",
      "    extraction: 0.12s\n",
      "  Actual writing time: 0.45s\n",
      "  Chunk total:    1.81s\n",
      "\n",
      "File transformation for QCI0Q completed in 21.43 seconds.\n",
      "=== Completed processing QCI0Q in 21.45 seconds ===\n"
     ]
    }
   ],
   "source": [
    "# Directory setup\n",
    "monitored_folder = ROOT_DIR / \"monitored_folder\"\n",
    "log_path = monitored_folder / \"detection_log.txt\"\n",
    "\n",
    "print(f\"Watching folder: {monitored_folder}\")\n",
    "\n",
    "\n",
    "# Updated wafer code extractor\n",
    "def extract_wafer_code(filename):\n",
    "    match = re.search(r\"(QC|QD|NV)[A-Z0-9]{3}\", filename.upper())\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "\n",
    "# Wait until file is readable (not still being written)\n",
    "def wait_for_file(filepath, retries=60, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with open(filepath, \"rb\"):\n",
    "                return True\n",
    "        except PermissionError:\n",
    "            print(f\"Waiting for file to be ready... (attempt {attempt + 1})\")\n",
    "            time.sleep(delay)\n",
    "    return False\n",
    "\n",
    "\n",
    "def initialise_spectra_processing(wafer_code, detection_time, file_path):\n",
    "    if wafer_code:\n",
    "        product_code = wafer_code[:2]\n",
    "        print(f\"Extracted wafer code: {wafer_code}\")\n",
    "\n",
    "        if product_code == \"QC\":\n",
    "            decoder_path = ROOT_DIR / \"decoders\" / SUBARU_DECODER\n",
    "        elif product_code in (\"QD\", \"NV\"):\n",
    "            decoder_path = ROOT_DIR / \"decoders\" / HALO_DECODER\n",
    "        else:\n",
    "            message = f\"Unsupported product code: {product_code}\"\n",
    "            print(message)\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{detection_time}] {message}\\n\")\n",
    "            return\n",
    "\n",
    "        decoder_df = load_decoder(decoder_path)\n",
    "        process_export_and_peaks(file_path, wafer_code, decoder_df)  # Main Spectra Processor\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"[{detection_time}] ✅ Processed successfully: {file_path.name} (Wafer: {wafer_code})\\n\")\n",
    "    else:\n",
    "        message = f\"No valid wafer code found in filename: {file_path.name}\"\n",
    "        print(message)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"[{detection_time}] ❌ {message}\\n\")\n",
    "\n",
    "\n",
    "# Handler for new files\n",
    "class WaferFileHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory or not event.src_path.endswith(\".csv\"):\n",
    "            return\n",
    "\n",
    "        file_path = Path(event.src_path)\n",
    "        detection_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Log detection immediately\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"[{detection_time}] Detected new file: {file_path.name}\\n\")\n",
    "\n",
    "        print(f\"\\nDetected new file: {file_path.name}\")\n",
    "\n",
    "        if not wait_for_file(file_path):\n",
    "            message = f\"File never became readable. Skipping: {file_path.name}\"\n",
    "            print(message)\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "                log_file.write(f\"[{detection_time}] {message}\\n\")\n",
    "            return\n",
    "\n",
    "        wafer_code = extract_wafer_code(file_path.name)\n",
    "\n",
    "        # Call Spectra Processing\n",
    "        initialise_spectra_processing(wafer_code, detection_time, file_path)\n",
    "\n",
    "\n",
    "# Watchdog setup\n",
    "observer = Observer()\n",
    "event_handler = WaferFileHandler()\n",
    "observer.schedule(event_handler, str(monitored_folder), recursive=False)\n",
    "observer.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    observer.stop()\n",
    "observer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental: Threading Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_RUN_NAME = \"larger\"\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, wavelength_lb=827, wavelength_ub=830, chunksize=1000, max_chunks=1000):\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns  # Read just headers\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "\n",
    "    # Extract wavelengths from column names\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "\n",
    "    # Filter columns to only include those within the desired range\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "\n",
    "    # Define columns to read\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break  # Stop after processing max_chunks\n",
    "\n",
    "            # Melt the dataframe: Convert wide format to long format\n",
    "            long_df = chunk.melt(\n",
    "                id_vars=[\"X\", \"Y\"],\n",
    "                value_vars=selected_intensity_cols,\n",
    "                var_name=\"Wavelength\",\n",
    "                value_name=\"Intensity\",\n",
    "            )\n",
    "\n",
    "            # Convert \"Wavelength\" column from \"Intensity_xxx\" to just \"xxx\"\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "\n",
    "            yield long_df  # Yield processed chunk instead of storing in memory\n",
    "\n",
    "\n",
    "def process_and_export(filepath, wafer_code, export_path, run_name):\n",
    "    try:\n",
    "        print(f\"Processing file: {filepath} for wafer: {wafer_code}\")\n",
    "        output_path = export_path / f\"{run_name}_threaded_{wafer_code}_spectra_formatted.csv\"\n",
    "        first_chunk = True\n",
    "\n",
    "        for transformed_chunk in transform_raw_file(filepath, wafer_code):\n",
    "            # export_start_time = time.time()\n",
    "            transformed_chunk.to_csv(output_path, mode=\"w\" if first_chunk else \"a\", header=first_chunk, index=False)\n",
    "            first_chunk = False  # Only write header for the first chunk\n",
    "            # print(f\"Chunk exported for wafer {wafer_code}. Time taken: {time.time() - export_start_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "def parallel_processing(filepaths, wafer_codes, export_path, run_name):\n",
    "    print(f\"Starting parallel processing with {len(filepaths)} files.\")\n",
    "    # Use ThreadPoolExecutor instead of ProcessPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_and_export, f, w, export_path, run_name): (f, w) for f, w in zip(filepaths, wafer_codes)\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            filepath, wafer_code = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exception raised in the process\n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {filepath}, wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "# CALLING THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    start_total_time = time.time()\n",
    "    print(\"\\n=== Starting full processing run with multiprocessing ===\\n\")\n",
    "    parallel_processing(file_paths, wafer_codes, EXPORTS_FILE_PATH, ANALYSIS_RUN_NAME)\n",
    "    end_total_time = time.time()\n",
    "    total_time = end_total_time - start_total_time\n",
    "    print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
