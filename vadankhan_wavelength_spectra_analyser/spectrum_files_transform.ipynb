{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# import threading\n",
    "# import multiprocessing\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())\n",
    "\n",
    "# Move to the root directory\n",
    "\n",
    "ROOT_DIR = CURRENT_DIR.parents[0]  # Adjust the number based on your folder structure\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "RAW_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "EXPORTS_FILE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add the root directory to the system path\n",
    "\n",
    "sys.path.append(str(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Raw File and Decoder File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('c:/Users/762093/Documents/vadankhan-wavelength-spectra-analyser/wavelength_spectra_files/QCHZS_spectra.csv')]\n"
     ]
    }
   ],
   "source": [
    "wafer_codes = [\"QCHZS\"]  # List of wafer codes\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ANALYSIS_RUN_NAME = \"POLARS_DEBUG\"\n",
    "\n",
    "SUBARU_DECODER = \"QC WAFER_LAYOUT 24Dec.csv\"\n",
    "HALO_DECODER = \"HALO_DECODER_NE-rev1_1 logic_coords_annotated.csv\"\n",
    "\n",
    "\n",
    "def find_files_with_wafer_codes(wafer_codes, raw_file_path):\n",
    "    # Initialize an empty list to store the paths of the files\n",
    "    file_paths = []\n",
    "    # Iterate over each wafer code in the list\n",
    "    for code in wafer_codes:\n",
    "        # Iterate over each file in the raw file path directory\n",
    "        for root, dirs, files in os.walk(raw_file_path):\n",
    "            for file in files:\n",
    "                # Check if the wafer code is in the file name\n",
    "                if code in file:\n",
    "                    # Add the full path of the file to the list\n",
    "                    file_paths.append(Path(root) / file)\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "# Call the function and print the results\n",
    "\n",
    "file_paths = find_files_with_wafer_codes(wafer_codes, RAW_FILE_PATH)\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting full processing run ===\n",
      "\n",
      "c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\exports\n",
      "\n",
      "--- Processing wafer: QCHZS (Product: QC) ---\n",
      "Loading decoder from: c:\\Users\\762093\\Documents\\vadankhan-wavelength-spectra-analyser\\decoders\\QC WAFER_LAYOUT 24Dec.csv\n",
      "Loaded in 0.74 seconds.\n",
      "\n",
      "\n",
      "=== Starting processing for QCHZS ===\n",
      "Starting file transformation for QCHZS...\n",
      "Header parsing and column filtering took 0.07 s\n",
      "Chunk 1 Summary:\n",
      "  Base transform: 0.78s\n",
      "  Peak detection: 21.83s\n",
      "  Conversion time: 18.15s\n",
      "  Actual writing time: 37.84s\n",
      "  Chunk total:    82.00s\n",
      "\n",
      "Chunk 2 Summary:\n",
      "  Base transform: 0.63s\n",
      "  Peak detection: 18.33s\n",
      "  Conversion time: 16.73s\n",
      "  Actual writing time: 32.99s\n",
      "  Chunk total:    71.64s\n",
      "\n",
      "Chunk 3 Summary:\n",
      "  Base transform: 0.49s\n",
      "  Peak detection: 17.76s\n",
      "  Conversion time: 16.01s\n",
      "  Actual writing time: 32.17s\n",
      "  Chunk total:    69.58s\n",
      "\n",
      "Chunk 4 Summary:\n",
      "  Base transform: 0.50s\n",
      "  Peak detection: 18.42s\n",
      "  Conversion time: 16.86s\n",
      "  Actual writing time: 32.75s\n",
      "  Chunk total:    71.53s\n",
      "\n",
      "Chunk 5 Summary:\n",
      "  Base transform: 0.55s\n",
      "  Peak detection: 18.56s\n",
      "  Conversion time: 17.16s\n",
      "  Actual writing time: 32.64s\n",
      "  Chunk total:    71.98s\n",
      "\n",
      "Chunk 6 Summary:\n",
      "  Base transform: 0.50s\n",
      "  Peak detection: 17.44s\n",
      "  Conversion time: 15.69s\n",
      "  Actual writing time: 31.58s\n",
      "  Chunk total:    68.30s\n",
      "\n",
      "Chunk 7 Summary:\n",
      "  Base transform: 0.49s\n",
      "  Peak detection: 17.66s\n",
      "  Conversion time: 15.74s\n",
      "  Actual writing time: 31.86s\n",
      "  Chunk total:    68.71s\n",
      "\n",
      "Chunk 8 Summary:\n",
      "  Base transform: 0.56s\n",
      "  Peak detection: 17.28s\n",
      "  Conversion time: 15.48s\n",
      "  Actual writing time: 31.46s\n",
      "  Chunk total:    67.83s\n",
      "\n",
      "Chunk 9 Summary:\n",
      "  Base transform: 0.65s\n",
      "  Peak detection: 17.45s\n",
      "  Conversion time: 15.67s\n",
      "  Actual writing time: 31.98s\n",
      "  Chunk total:    68.82s\n",
      "\n",
      "Chunk 10 Summary:\n",
      "  Base transform: 0.51s\n",
      "  Peak detection: 17.42s\n",
      "  Conversion time: 15.46s\n",
      "  Actual writing time: 31.64s\n",
      "  Chunk total:    68.05s\n",
      "\n",
      "File transformation for QCHZS completed in 722.57 seconds.\n",
      "=== Completed processing QCHZS in 722.57 seconds ===\n",
      "\n",
      "=== Total processing time: 723.34 seconds ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decoder loading function\n",
    "def load_decoder(decoder_file_path):\n",
    "    print(f\"Loading decoder from: {decoder_file_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not decoder_file_path.exists():\n",
    "        print(f\"Decoder file not found at {decoder_file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_decoder = pd.read_csv(decoder_file_path, usecols=[\"Logic_X\", \"Logic_Y\", \"TE_LABEL\", \"TYPE\"])\n",
    "    df_decoder = df_decoder.set_index([\"Logic_X\", \"Logic_Y\"])\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Loaded in {end_time - start_time:.2f} seconds.\\n\")\n",
    "    return df_decoder\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, decoder_df, wavelength_lb=824, wavelength_ub=832, chunksize=10000, max_chunks=10):\n",
    "    print(f\"Starting file transformation for {wafer_id}...\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    t1 = time.time()\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "    data_points_threshold = len(selected_intensity_cols)\n",
    "    print(f\"Header parsing and column filtering took {time.time() - t1:.2f} s\")\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break\n",
    "\n",
    "            chunk_start = time.time()\n",
    "\n",
    "            # Base transformation\n",
    "            t_base = time.time()\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "            long_df = long_df.merge(decoder_df, left_on=[\"X\", \"Y\"], right_index=True, how=\"left\")\n",
    "            long_df = long_df.drop(columns=[\"X\", \"Y\"])\n",
    "            long_df = long_df[[\"TYPE\", \"TE_LABEL\", \"Wavelength\", \"Intensity\"]]\n",
    "            t_base_elapsed = time.time() - t_base\n",
    "\n",
    "            yield long_df, data_points_threshold, t_base_elapsed\n",
    "\n",
    "            # print(f\"Chunk {i+1} | Base transform: {t_base_elapsed:.2f}s | Total time: {time.time() - chunk_start:.2f}s\")\n",
    "\n",
    "    print(f\"File transformation for {wafer_id} completed in {time.time() - total_t0:.2f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_top_two_peaks(df_group):\n",
    "    \"\"\"\n",
    "    Detects the top two peaks in a spectrum.\n",
    "    - SMSR_dB is calculated as -secondary_peak_dB (with main peak at 0 dB)\n",
    "    - SMSR_linear is calculated from the ratio of linear intensities\n",
    "    \"\"\"\n",
    "    df_sorted = df_group.sort_values(\"Wavelength\")\n",
    "    intensities = df_sorted[\"Intensity\"].values\n",
    "    dB_intensities = df_sorted[\"dB_Intensity\"].values\n",
    "    wavelengths = df_sorted[\"Wavelength\"].values\n",
    "\n",
    "    peak_indices, _ = find_peaks(dB_intensities)\n",
    "    if len(peak_indices) == 0:\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"highest_peak_wavelength\": np.nan,\n",
    "                \"highest_peak_intensity_linear\": np.nan,\n",
    "                \"second_peak_wavelength\": np.nan,\n",
    "                \"second_peak_intensity_linear\": np.nan,\n",
    "                \"SMSR_dB\": np.nan,\n",
    "                \"SMSR_linear\": np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Sort peaks by dB intensity descending\n",
    "    sorted_order = np.argsort(dB_intensities[peak_indices])[::-1]\n",
    "    highest_idx = peak_indices[sorted_order[0]]\n",
    "    highest_peak_wavelength = wavelengths[highest_idx]\n",
    "    highest_peak_intensity_linear = intensities[highest_idx]\n",
    "\n",
    "    if len(sorted_order) > 1:\n",
    "        second_idx = peak_indices[sorted_order[1]]\n",
    "        second_peak_wavelength = wavelengths[second_idx]\n",
    "        second_peak_intensity_linear = intensities[second_idx]\n",
    "        second_peak_dB = dB_intensities[second_idx]\n",
    "\n",
    "        SMSR_dB = -second_peak_dB\n",
    "        SMSR_linear = highest_peak_intensity_linear / second_peak_intensity_linear\n",
    "    else:\n",
    "        second_peak_wavelength = np.nan\n",
    "        second_peak_intensity_linear = np.nan\n",
    "        SMSR_dB = np.nan\n",
    "        SMSR_linear = np.nan\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"highest_peak_wavelength\": highest_peak_wavelength,\n",
    "            \"highest_peak_intensity_linear\": highest_peak_intensity_linear,\n",
    "            \"second_peak_wavelength\": second_peak_wavelength,\n",
    "            \"second_peak_intensity_linear\": second_peak_intensity_linear,\n",
    "            \"SMSR_dB\": SMSR_dB,\n",
    "            \"SMSR_linear\": SMSR_linear,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def process_export_and_peaks(filepath, wafer_code, decoder_df):\n",
    "    print(f\"\\n=== Starting processing for {wafer_code} ===\")\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    spectra_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    peak_output_path = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_peaks_summary.csv\"\n",
    "    first_chunk = True\n",
    "    first_peak = True\n",
    "\n",
    "    accumulator = {}\n",
    "    data_point_count = {}\n",
    "\n",
    "    chunk_counter = 0\n",
    "\n",
    "    for chunk, data_points_threshold, base_time in transform_raw_file(filepath, wafer_code, decoder_df):\n",
    "        chunk_counter += 1\n",
    "        chunk_start = time.time()\n",
    "\n",
    "        t_peaks_total = 0\n",
    "        t_conversion_total = 0\n",
    "        t_actual_write_total = 0\n",
    "\n",
    "        for te_label, group in chunk.groupby(\"TE_LABEL\"):\n",
    "            if te_label not in accumulator:\n",
    "                accumulator[te_label] = [group]\n",
    "                data_point_count[te_label] = len(group)\n",
    "            else:\n",
    "                accumulator[te_label].append(group)\n",
    "                data_point_count[te_label] += len(group)\n",
    "\n",
    "            if data_point_count[te_label] >= data_points_threshold:\n",
    "                full_data = pd.concat(accumulator[te_label], ignore_index=True)\n",
    "\n",
    "                # Peak processing\n",
    "                t_peaks = time.time()\n",
    "                max_intensity = full_data[\"Intensity\"].max()\n",
    "                full_data[\"dB_Intensity\"] = 10 * np.log10(full_data[\"Intensity\"] / max_intensity)\n",
    "                peak_series = extract_top_two_peaks(full_data)\n",
    "                peak_series[\"TE_LABEL\"] = te_label\n",
    "                t_peaks_total += time.time() - t_peaks\n",
    "\n",
    "                # Conversion\n",
    "                t_conversion_start = time.time()\n",
    "                table_full_data = pa.Table.from_pandas(full_data)\n",
    "                table_peak_series = pa.Table.from_pandas(pd.DataFrame([peak_series]))\n",
    "                t_conversion_end = time.time()\n",
    "                t_conversion_total += t_conversion_end - t_conversion_start\n",
    "\n",
    "                # Actual writing\n",
    "                t_actual_write_start = time.time()\n",
    "                csv.write_csv(table_full_data, spectra_output_path, write_options=csv.WriteOptions(include_header=first_chunk))\n",
    "                csv.write_csv(table_peak_series, peak_output_path, write_options=csv.WriteOptions(include_header=first_peak))\n",
    "                t_actual_write_end = time.time()\n",
    "                t_actual_write_total += t_actual_write_end - t_actual_write_start\n",
    "\n",
    "                first_chunk = False\n",
    "                first_peak = False\n",
    "\n",
    "                del accumulator[te_label]\n",
    "                del data_point_count[te_label]\n",
    "\n",
    "        chunk_total = time.time() - chunk_start\n",
    "        print(f\"Chunk {chunk_counter} Summary:\")\n",
    "        print(f\"  Base transform: {base_time:.2f}s\")\n",
    "        print(f\"  Peak detection: {t_peaks_total:.2f}s\")\n",
    "        print(f\"  Conversion time: {t_conversion_total:.2f}s\")\n",
    "        print(f\"  Actual writing time: {t_actual_write_total:.2f}s\")\n",
    "        print(f\"  Chunk total:    {chunk_total:.2f}s\\n\")\n",
    "\n",
    "    print(f\"=== Completed processing {wafer_code} in {time.time() - total_t0:.2f} seconds ===\")\n",
    "\n",
    "\n",
    "def get_columns_in_wavelength_range(filepath, wavelength_lb, wavelength_ub):\n",
    "    # Read just the header row\n",
    "    with open(filepath, \"r\") as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "\n",
    "    # Find columns that are within the desired wavelength range\n",
    "    selected_intensity_cols = [col for col in header if col.startswith(\"Intensity_\") and wavelength_lb <= float(col.split(\"_\")[1]) <= wavelength_ub]\n",
    "\n",
    "    # Add essential metadata columns\n",
    "    return [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "\n",
    "# Main execution\n",
    "start_total_time = time.time()\n",
    "print(\"\\n=== Starting full processing run ===\\n\")\n",
    "\n",
    "print(EXPORTS_FILE_PATH)\n",
    "\n",
    "# file_paths should be defined elsewhere, assumed to match wafer_codes order\n",
    "for filepath, wafer_code in zip(file_paths, wafer_codes):\n",
    "    # Extract product code from the first two characters of the wafer code\n",
    "    product_code = wafer_code[:2]\n",
    "\n",
    "    print(f\"\\n--- Processing wafer: {wafer_code} (Product: {product_code}) ---\")\n",
    "\n",
    "    # Select decoder file based on product code\n",
    "    if product_code == \"QC\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / SUBARU_DECODER\n",
    "    elif product_code == \"QD\" or \"NV\":\n",
    "        decoder_path = ROOT_DIR / \"decoders\" / HALO_DECODER\n",
    "    else:\n",
    "        print(f\"Unsupported product code: {product_code}\")\n",
    "        continue\n",
    "\n",
    "    # Load decoder and process\n",
    "    decoder_dict = load_decoder(decoder_path)\n",
    "    process_export_and_peaks(filepath, wafer_code, decoder_dict)\n",
    "\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_time = end_total_time - start_total_time\n",
    "\n",
    "\n",
    "print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Sort in Python (slower than JMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final output file by TE_LABEL\n",
    "def sort_large_csv_with_dask(input_path, output_path):\n",
    "    print(f\"Starting Dask sort for: {input_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read CSV with Dask\n",
    "    df = dd.read_csv(input_path, assume_missing=True)  # assume_missing=True is safe for mixed data\n",
    "\n",
    "    # Sort by TE_LABEL first, then Wavelength\n",
    "    df_sorted = df.sort_values(by=[\"TE_LABEL\", \"Wavelength\"])\n",
    "\n",
    "    # Save to CSV (can write to multiple files if very large)\n",
    "    df_sorted.to_csv(output_path, index=False, single_file=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Sorting and export completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# Call for each wafer (in case of multiple)\n",
    "for wafer_code in wafer_codes:\n",
    "    input_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_spectra_formatted.csv\"\n",
    "    output_csv = EXPORTS_FILE_PATH / f\"{ANALYSIS_RUN_NAME}_{wafer_code}_sorted.csv\"\n",
    "    sort_large_csv_with_dask(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental: Threading Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_RUN_NAME = \"larger\"\n",
    "\n",
    "RAW_FILE_PATH = ROOT_DIR / \"wavelength_spectra_files\"\n",
    "EXPORTS_FILE_PATH = ROOT_DIR / \"exports\"\n",
    "\n",
    "\n",
    "def transform_raw_file(filepath, wafer_id, wavelength_lb=827, wavelength_ub=830, chunksize=1000, max_chunks=1000):\n",
    "    col_names = pd.read_csv(filepath, nrows=1).columns  # Read just headers\n",
    "    intensity_cols = [col for col in col_names if col.startswith(\"Intensity_\")]\n",
    "\n",
    "    # Extract wavelengths from column names\n",
    "    wavelengths = {col: float(col.split(\"_\")[1]) for col in intensity_cols}\n",
    "\n",
    "    # Filter columns to only include those within the desired range\n",
    "    selected_intensity_cols = [col for col, wl in wavelengths.items() if wavelength_lb <= wl <= wavelength_ub]\n",
    "\n",
    "    # Define columns to read\n",
    "    usecols = [\"X\", \"Y\"] + selected_intensity_cols\n",
    "\n",
    "    with pd.read_csv(filepath, chunksize=chunksize, usecols=usecols) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            if i >= max_chunks:\n",
    "                break  # Stop after processing max_chunks\n",
    "\n",
    "            # Melt the dataframe: Convert wide format to long format\n",
    "            long_df = chunk.melt(id_vars=[\"X\", \"Y\"], value_vars=selected_intensity_cols, var_name=\"Wavelength\", value_name=\"Intensity\")\n",
    "\n",
    "            # Convert \"Wavelength\" column from \"Intensity_xxx\" to just \"xxx\"\n",
    "            long_df[\"Wavelength\"] = long_df[\"Wavelength\"].map(wavelengths)\n",
    "\n",
    "            yield long_df  # Yield processed chunk instead of storing in memory\n",
    "\n",
    "\n",
    "def process_and_export(filepath, wafer_code, export_path, run_name):\n",
    "    try:\n",
    "        print(f\"Processing file: {filepath} for wafer: {wafer_code}\")\n",
    "        output_path = export_path / f\"{run_name}_threaded_{wafer_code}_spectra_formatted.csv\"\n",
    "        first_chunk = True\n",
    "\n",
    "        for transformed_chunk in transform_raw_file(filepath, wafer_code):\n",
    "            # export_start_time = time.time()\n",
    "            transformed_chunk.to_csv(output_path, mode=\"w\" if first_chunk else \"a\", header=first_chunk, index=False)\n",
    "            first_chunk = False  # Only write header for the first chunk\n",
    "            # print(f\"Chunk exported for wafer {wafer_code}. Time taken: {time.time() - export_start_time:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "def parallel_processing(filepaths, wafer_codes, export_path, run_name):\n",
    "    print(f\"Starting parallel processing with {len(filepaths)} files.\")\n",
    "    # Use ThreadPoolExecutor instead of ProcessPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_and_export, f, w, export_path, run_name): (f, w) for f, w in zip(filepaths, wafer_codes)}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            filepath, wafer_code = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise any exception raised in the process\n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {filepath}, wafer {wafer_code}: {e}\")\n",
    "\n",
    "\n",
    "# CALLING THE CODE\n",
    "if __name__ == \"__main__\":\n",
    "    start_total_time = time.time()\n",
    "    print(\"\\n=== Starting full processing run with multiprocessing ===\\n\")\n",
    "    parallel_processing(file_paths, wafer_codes, EXPORTS_FILE_PATH, ANALYSIS_RUN_NAME)\n",
    "    end_total_time = time.time()\n",
    "    total_time = end_total_time - start_total_time\n",
    "    print(f\"\\n=== Total processing time: {total_time:.2f} seconds ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: Spectrum Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sweep_data(df_raw_sweeps, wafer_code):\n",
    "    cod_roll_eval_types = [\"COD\", \"ROLLOVER\", \"NO LASER\"]\n",
    "    colors = [\"red\", \"blue\", \"orange\"]\n",
    "\n",
    "    # Scatter plot of PD against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"PD\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of PD vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"PD\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot DP/DI against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"DP/DI\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of DP/DI vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"DP/DI\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of Vf against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"Vf\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of Vf vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"Vf\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot dV/dI against LDI_mA with subplots for each COD_ROLL_EVAL type\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    for ax, cod_type, color in zip(axs, cod_roll_eval_types, colors):\n",
    "        group = df_raw_sweeps[df_raw_sweeps[\"COD_ROLL_EVAL\"] == cod_type]\n",
    "        ax.scatter(group[\"LDI_mA\"], group[\"dV/dI\"], alpha=0.8, s=0.2, color=color)\n",
    "        ax.set_title(f\"Scatter Plot of dV/dI vs LDI_mA for {cod_type} (Wafer Code: {wafer_code})\")\n",
    "        ax.set_xlabel(\"LDI_mA\")\n",
    "        ax.set_ylabel(\"dV/dI\")\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calling Code\n",
    "for df_raw_sweeps in annotated_sweeps_tables:\n",
    "    wafer_code = df_raw_sweeps[\"WAFER_ID\"].iloc[0]\n",
    "    plot_sweep_data(df_raw_sweeps, wafer_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
